<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>entropy.entropy &#8212; entropy 0.1.0 documentation</title>
    <link rel="stylesheet" href="../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/style.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/copybutton.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../../_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../../_static/bootstrap-sphinx.js"></script>
    <link rel="shortcut icon" href="../../_static/entropy.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../index.html"><span><img src="../../_static/entropy_128x128.png"></span>
          entropy</a>
        <span class="navbar-text navbar-version pull-left"><b>0.1.0</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../../api.html">API</a></li>
                <li><a href="../../changelog.html">What's new</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"></ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12 content">
      
  <h1>Source code for entropy.entropy</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="k">import</span> <span class="n">factorial</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">KDTree</span>
<span class="kn">from</span> <span class="nn">scipy.signal</span> <span class="k">import</span> <span class="n">periodogram</span><span class="p">,</span> <span class="n">welch</span>

<span class="kn">from</span> <span class="nn">.utils</span> <span class="k">import</span> <span class="n">_embed</span>

<span class="nb">all</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;perm_entropy&#39;</span><span class="p">,</span> <span class="s1">&#39;spectral_entropy&#39;</span><span class="p">,</span> <span class="s1">&#39;svd_entropy&#39;</span><span class="p">,</span> <span class="s1">&#39;app_entropy&#39;</span><span class="p">,</span>
       <span class="s1">&#39;sample_entropy&#39;</span><span class="p">]</span>


<div class="viewcode-block" id="perm_entropy"><a class="viewcode-back" href="../../generated/entropy.perm_entropy.html#entropy.perm_entropy">[docs]</a><span class="k">def</span> <span class="nf">perm_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Permutation Entropy.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : list or np.array</span>
<span class="sd">        One-dimensional time series of shape (n_times)</span>
<span class="sd">    order : int</span>
<span class="sd">        Order of permutation entropy</span>
<span class="sd">    delay : int</span>
<span class="sd">        Time delay</span>
<span class="sd">    normalize : bool</span>
<span class="sd">        If True, divide by log2(order!) to normalize the entropy between 0</span>
<span class="sd">        and 1. Otherwise, return the permutation entropy in bit.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pe : float</span>
<span class="sd">        Permutation Entropy</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The permutation entropy is a complexity measure for time-series first</span>
<span class="sd">    introduced by Bandt and Pompe in 2002 [1]_.</span>

<span class="sd">    The permutation entropy of a signal :math:`x` is defined as:</span>

<span class="sd">    .. math:: H = -\sum p(\pi)log_2(\pi)</span>

<span class="sd">    where the sum runs over all :math:`n!` permutations :math:`\pi` of order</span>
<span class="sd">    :math:`n`. This is the information contained in comparing :math:`n`</span>
<span class="sd">    consecutive values of the time series. It is clear that</span>
<span class="sd">    :math:`0 ≤ H (n) ≤ log_2(n!)` where the lower bound is attained for an</span>
<span class="sd">    increasing or decreasing sequence of values, and the upper bound for a</span>
<span class="sd">    completely random system where all :math:`n!` possible permutations appear</span>
<span class="sd">    with the same probability.</span>

<span class="sd">    The embedded matrix :math:`Y` is created by:</span>

<span class="sd">    .. math:: y(i)=[x_i,x_{i+delay}, ...,x_{i+(order-1) * delay}]</span>

<span class="sd">    .. math:: Y=[y(1),y(2),...,y(N-(order-1))*delay)]^T</span>


<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Bandt, Christoph, and Bernd Pompe. &quot;Permutation entropy: a</span>
<span class="sd">           natural complexity measure for time series.&quot; Physical review letters</span>
<span class="sd">           88.17 (2002): 174102.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    1. Permutation entropy with order 2</span>

<span class="sd">        &gt;&gt;&gt; from entropy import perm_entropy</span>
<span class="sd">        &gt;&gt;&gt; x = [4, 7, 9, 10, 6, 11, 3]</span>
<span class="sd">        &gt;&gt;&gt; # Return a value in bit between 0 and log2(factorial(order))</span>
<span class="sd">        &gt;&gt;&gt; print(perm_entropy(x, order=2))</span>
<span class="sd">            0.918</span>
<span class="sd">    2. Normalized permutation entropy with order 3</span>

<span class="sd">        &gt;&gt;&gt; from entropy import perm_entropy</span>
<span class="sd">        &gt;&gt;&gt; x = [4, 7, 9, 10, 6, 11, 3]</span>
<span class="sd">        &gt;&gt;&gt; # Return a value comprised between 0 and 1.</span>
<span class="sd">        &gt;&gt;&gt; print(perm_entropy(x, order=3, normalize=True))</span>
<span class="sd">            0.589</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ran_order</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">order</span><span class="p">)</span>
    <span class="n">hashmult</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="n">ran_order</span><span class="p">)</span>
    <span class="c1"># Embed x and sort the order of permutations</span>
    <span class="n">sorted_idx</span> <span class="o">=</span> <span class="n">_embed</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">)</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;quicksort&#39;</span><span class="p">)</span>
    <span class="c1"># Associate unique integer to each permutations</span>
    <span class="n">hashval</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">sorted_idx</span><span class="p">,</span> <span class="n">hashmult</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Return the counts</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">hashval</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Use np.true_divide for Python 2 compatibility</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">true_divide</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">c</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="n">pe</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">factorial</span><span class="p">(</span><span class="n">order</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pe</span></div>


<div class="viewcode-block" id="spectral_entropy"><a class="viewcode-back" href="../../generated/entropy.spectral_entropy.html#entropy.spectral_entropy">[docs]</a><span class="k">def</span> <span class="nf">spectral_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sf</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;fft&#39;</span><span class="p">,</span> <span class="n">nperseg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Spectral Entropy.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : list or np.array</span>
<span class="sd">        One-dimensional time series of shape (n_times)</span>
<span class="sd">    sf : float</span>
<span class="sd">        Sampling frequency</span>
<span class="sd">    method : str</span>
<span class="sd">        Spectral estimation method ::</span>

<span class="sd">        &#39;fft&#39; : Fourier Transform (via scipy.signal.periodogram)</span>
<span class="sd">        &#39;welch&#39; : Welch periodogram (via scipy.signal.welch)</span>

<span class="sd">    nperseg : str or int</span>
<span class="sd">        Length of each FFT segment for Welch method.</span>
<span class="sd">        If None, uses scipy default of 256 samples.</span>
<span class="sd">    normalize : bool</span>
<span class="sd">        If True, divide by log2(psd.size) to normalize the spectral entropy</span>
<span class="sd">        between 0 and 1. Otherwise, return the spectral entropy in bit.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    se : float</span>
<span class="sd">        Spectral Entropy</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Spectral Entropy is defined to be the Shannon Entropy of the Power</span>
<span class="sd">    Spectral Density (PSD) of the data:</span>

<span class="sd">    .. math:: H(x, sf) =  -\sum_{f=0}^{f_s/2} PSD(f) log_2[PSD(f)]</span>

<span class="sd">    Where :math:`PSD` is the normalised PSD, and :math:`f_s` is the sampling</span>
<span class="sd">    frequency.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Inouye, T. et al. (1991). Quantification of EEG irregularity by</span>
<span class="sd">       use of the entropy of the power spectrum. Electroencephalography</span>
<span class="sd">       and clinical neurophysiology, 79(3), 204-210.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    1. Spectral entropy of a pure sine using FFT</span>

<span class="sd">        &gt;&gt;&gt; from entropy import spectral_entropy</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; sf, f, dur = 100, 1, 4</span>
<span class="sd">        &gt;&gt;&gt; N = sf * duration # Total number of discrete samples</span>
<span class="sd">        &gt;&gt;&gt; t = np.arange(N) / sf # Time vector</span>
<span class="sd">        &gt;&gt;&gt; x = np.sin(2 * np.pi * f * t)</span>
<span class="sd">        &gt;&gt;&gt; print(np.round(spectral_entropy(x, sf, method=&#39;fft&#39;), 2)</span>
<span class="sd">            0.0</span>

<span class="sd">    2. Spectral entropy of a random signal using Welch&#39;s method</span>

<span class="sd">        &gt;&gt;&gt; from entropy import spectral_entropy</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; np.random.seed(42)</span>
<span class="sd">        &gt;&gt;&gt; x = np.random.rand(3000)</span>
<span class="sd">        &gt;&gt;&gt; print(spectral_entropy(x, sf=100, method=&#39;welch&#39;))</span>
<span class="sd">            9.939</span>

<span class="sd">    3. Normalized spectral entropy</span>

<span class="sd">        &gt;&gt;&gt; print(spectral_entropy(x, sf=100, method=&#39;welch&#39;, normalize=True))</span>
<span class="sd">            0.995</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># Compute and normalize power spectrum</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;fft&#39;</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">psd</span> <span class="o">=</span> <span class="n">periodogram</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sf</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;welch&#39;</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">psd</span> <span class="o">=</span> <span class="n">welch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sf</span><span class="p">,</span> <span class="n">nperseg</span><span class="o">=</span><span class="n">nperseg</span><span class="p">)</span>
    <span class="n">psd_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">psd</span><span class="p">,</span> <span class="n">psd</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
    <span class="n">se</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">psd_norm</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">psd_norm</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="n">se</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">psd_norm</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">se</span></div>


<div class="viewcode-block" id="svd_entropy"><a class="viewcode-back" href="../../generated/entropy.svd_entropy.html#entropy.svd_entropy">[docs]</a><span class="k">def</span> <span class="nf">svd_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Singular Value Decomposition entropy.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : list or np.array</span>
<span class="sd">        One-dimensional time series of shape (n_times)</span>
<span class="sd">    order : int</span>
<span class="sd">        Order of permutation entropy</span>
<span class="sd">    delay : int</span>
<span class="sd">        Time delay</span>
<span class="sd">    normalize : bool</span>
<span class="sd">        If True, divide by log2(order!) to normalize the entropy between 0</span>
<span class="sd">        and 1. Otherwise, return the permutation entropy in bit.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    svd_e : float</span>
<span class="sd">        SVD Entropy</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    SVD entropy is an indicator of the number of eigenvectors that are needed</span>
<span class="sd">    for an adequate explanation of the data set. In other words, it measures</span>
<span class="sd">    the dimensionality of the data.</span>

<span class="sd">    The SVD entropy of a signal :math:`x` is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        H = -\sum_{i=1}^{M} \overline{\sigma}_i log_2(\overline{\sigma}_i)</span>

<span class="sd">    where :math:`M` is the number of singular values of the embedded matrix</span>
<span class="sd">    :math:`Y` and :math:`\sigma_1, \sigma_2, ..., \sigma_M` are the</span>
<span class="sd">    normalized singular values of :math:`Y`.</span>

<span class="sd">    The embedded matrix :math:`Y` is created by:</span>

<span class="sd">    .. math:: y(i)=[x_i,x_{i+delay}, ...,x_{i+(order-1) * delay}]</span>

<span class="sd">    .. math:: Y=[y(1),y(2),...,y(N-(order-1))*delay)]^T</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    1. SVD entropy with order 2</span>

<span class="sd">        &gt;&gt;&gt; from entropy import svd_entropy</span>
<span class="sd">        &gt;&gt;&gt; x = [4, 7, 9, 10, 6, 11, 3]</span>
<span class="sd">        &gt;&gt;&gt; # Return a value in bit between 0 and log2(factorial(order))</span>
<span class="sd">        &gt;&gt;&gt; print(svd_entropy(x, order=2))</span>
<span class="sd">            0.762</span>

<span class="sd">    2. Normalized SVD entropy with order 3</span>

<span class="sd">        &gt;&gt;&gt; from entropy import svd_entropy</span>
<span class="sd">        &gt;&gt;&gt; x = [4, 7, 9, 10, 6, 11, 3]</span>
<span class="sd">        &gt;&gt;&gt; # Return a value comprised between 0 and 1.</span>
<span class="sd">        &gt;&gt;&gt; print(svd_entropy(x, order=3, normalize=True))</span>
<span class="sd">            0.687</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">mat</span> <span class="o">=</span> <span class="n">_embed</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">)</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># Normalize the singular values</span>
    <span class="n">W</span> <span class="o">/=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
    <span class="n">svd_e</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">W</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="n">svd_e</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">order</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">svd_e</span></div>


<span class="k">def</span> <span class="nf">_app_samp_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;chebyshev&#39;</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Utility function for `app_entropy`` and `sample_entropy`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_all_metrics</span> <span class="o">=</span> <span class="n">KDTree</span><span class="o">.</span><span class="n">valid_metrics</span>
    <span class="k">if</span> <span class="n">metric</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_all_metrics</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The given metric (</span><span class="si">%s</span><span class="s1">) is not valid. The valid &#39;</span>
                         <span class="s1">&#39;metric names are: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="n">_all_metrics</span><span class="p">))</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># compute phi(order, r)</span>
    <span class="n">_emb_data1</span> <span class="o">=</span> <span class="n">_embed</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">approximate</span><span class="p">:</span>
        <span class="n">emb_data1</span> <span class="o">=</span> <span class="n">_emb_data1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">emb_data1</span> <span class="o">=</span> <span class="n">_emb_data1</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">count1</span> <span class="o">=</span> <span class="n">KDTree</span><span class="p">(</span><span class="n">emb_data1</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">)</span><span class="o">.</span><span class="n">query_radius</span><span class="p">(</span><span class="n">emb_data1</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span>
                                                           <span class="n">count_only</span><span class="o">=</span><span class="kc">True</span>
                                                           <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="c1"># compute phi(order + 1, r)</span>
    <span class="n">emb_data2</span> <span class="o">=</span> <span class="n">_embed</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">count2</span> <span class="o">=</span> <span class="n">KDTree</span><span class="p">(</span><span class="n">emb_data2</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">)</span><span class="o">.</span><span class="n">query_radius</span><span class="p">(</span><span class="n">emb_data2</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span>
                                                           <span class="n">count_only</span><span class="o">=</span><span class="kc">True</span>
                                                           <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">approximate</span><span class="p">:</span>
        <span class="n">phi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">count1</span> <span class="o">/</span> <span class="n">emb_data1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">phi</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">count2</span> <span class="o">/</span> <span class="n">emb_data2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">phi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">count1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">emb_data1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">phi</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">count2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">emb_data2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">phi</span>


<div class="viewcode-block" id="app_entropy"><a class="viewcode-back" href="../../generated/entropy.app_entropy.html#entropy.app_entropy">[docs]</a><span class="k">def</span> <span class="nf">app_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;chebyshev&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Approximate Entropy.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : list or np.array</span>
<span class="sd">        One-dimensional time series of shape (n_times)</span>
<span class="sd">    order : int (default: 2)</span>
<span class="sd">        Embedding dimension.</span>
<span class="sd">    metric : str (default: chebyshev)</span>
<span class="sd">        Name of the metric function used with</span>
<span class="sd">        :class:`~sklearn.neighbors.KDTree`. The list of available</span>
<span class="sd">        metric functions is given by: ``KDTree.valid_metrics``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ae : float</span>
<span class="sd">        Approximate Entropy.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Original code from the mne-features package.</span>

<span class="sd">    Approximate entropy is a technique used to quantify the amount of</span>
<span class="sd">    regularity and the unpredictability of fluctuations over time-series data.</span>

<span class="sd">    Smaller values indicates that the data is more regular and predictable.</span>

<span class="sd">    The value of :math:`r` is set to :math:`0.2 * std(x)`.</span>

<span class="sd">    Code adapted from the mne-features package by Jean-Baptiste Schiratti</span>
<span class="sd">    and Alexandre Gramfort.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Richman, J. S. et al. (2000). Physiological time-series analysis</span>
<span class="sd">           using approximate entropy and sample entropy. American Journal of</span>
<span class="sd">           Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.</span>

<span class="sd">    1. Approximate entropy with order 2.</span>

<span class="sd">        &gt;&gt;&gt; from entropy import app_entropy</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; np.random.seed(1234567)</span>
<span class="sd">        &gt;&gt;&gt; x = np.random.rand(3000)</span>
<span class="sd">        &gt;&gt;&gt; print(app_entropy(x, order=2))</span>
<span class="sd">            2.075</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="n">_app_samp_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">phi</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">phi</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span></div>


<div class="viewcode-block" id="sample_entropy"><a class="viewcode-back" href="../../generated/entropy.sample_entropy.html#entropy.sample_entropy">[docs]</a><span class="k">def</span> <span class="nf">sample_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;chebyshev&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sample Entropy.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : list or np.array</span>
<span class="sd">        One-dimensional time series of shape (n_times)</span>
<span class="sd">    order : int (default: 2)</span>
<span class="sd">        Embedding dimension.</span>
<span class="sd">    metric : str (default: chebyshev)</span>
<span class="sd">        Name of the metric function used with KDTree. The list of available</span>
<span class="sd">        metric functions is given by: `KDTree.valid_metrics`.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    se : float</span>
<span class="sd">        Sample Entropy.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Original code from the mne-features package.</span>

<span class="sd">    Sample entropy is a modification of approximate entropy, used for assessing</span>
<span class="sd">    the complexity of physiological time-series signals. It has two advantages</span>
<span class="sd">    over approximate entropy: data length independence and a relatively</span>
<span class="sd">    trouble-free implementation. Large values indicate high complexity whereas</span>
<span class="sd">    smaller values characterize more self-similar and regular signals.</span>

<span class="sd">    Sample entropy of a signal :math:`x` is defined as:</span>

<span class="sd">    .. math:: H(x, m, r) = -log\dfrac{C(m + 1, r)}{C(m, r)}</span>

<span class="sd">    where :math:`m` is the embedding dimension (= order), :math:`r` is</span>
<span class="sd">    the radius of the neighbourhood (default = :math:`0.2 * std(x)`),</span>
<span class="sd">    :math:`C(m + 1, r)` is the number of embedded vectors of length</span>
<span class="sd">    :math:`m + 1` having a Chebyshev distance inferior to :math:`r` and</span>
<span class="sd">    :math:`C(m, r)` is the number of embedded vectors of length</span>
<span class="sd">    :math:`m` having a Chebyshev distance inferior to :math:`r`.</span>

<span class="sd">    Code adapted from the mne-features package by Jean-Baptiste Schiratti</span>
<span class="sd">    and Alexandre Gramfort.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Richman, J. S. et al. (2000). Physiological time-series analysis</span>
<span class="sd">           using approximate entropy and sample entropy. American Journal of</span>
<span class="sd">           Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    1. Sample entropy with order 2.</span>

<span class="sd">        &gt;&gt;&gt; from entropy import sample_entropy</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; np.random.seed(1234567)</span>
<span class="sd">        &gt;&gt;&gt; x = np.random.rand(3000)</span>
<span class="sd">        &gt;&gt;&gt; print(sample_entropy(x, order=2))</span>
<span class="sd">            2.192</span>

<span class="sd">    2. Sample entropy with order 3 using the Euclidean distance.</span>

<span class="sd">        &gt;&gt;&gt; from entropy import sample_entropy</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; np.random.seed(1234567)</span>
<span class="sd">        &gt;&gt;&gt; x = np.random.rand(3000)</span>
<span class="sd">        &gt;&gt;&gt; print(sample_entropy(x, order=3, metric=&#39;euclidean&#39;))</span>
<span class="sd">            2.725</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="n">_app_samp_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">phi</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">phi</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Sample Entropy is not defined.&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">phi</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">phi</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span></div>
</pre></div>

    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
      
    </p>
    <p>
        &copy; Copyright 2018-2018, Raphael Vallat.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.8.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>